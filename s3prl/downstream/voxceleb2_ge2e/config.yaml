
# this voxceleb2 is doing speaker verification task!
runner:
  total_steps: 100000
  gradient_clipping: 1
  gradient_accumulate_steps: 1

  log_step: 100
  eval_step: 2000
  save_step: 2000
  max_keep: 1
  eval_dataloaders: 
    - dev
    - test
  
optimizer: 
  name: AdamW
  lr: 4.0e-4

# comment the whole scheduler config block to disable learning rate scheduling
scheduler:
  name: linear_schedule_with_warmup
  num_warmup_steps: 1000

downstream_expert: 
  # we need to split data into three part
  datarc:
    train:
      file_path: 
        Voxceleb1: ../librispeech/vox1_train_verifi/wav
        Voxceleb2: ../librispeech/vox2_dev/wav
      meta_data: ./downstream/speaker_verifi/dev_meta_data/dev_speaker_ids.txt
      utter_number: 10
      max_timestep: 128000
    
    test:
      file_path: ../librispeech/vox1_test/wav
      meta_data: /home/pohan/data/librispeech/vox1_test/veri_test.txt
    
    dev:
      file_path: ../librispeech/vox1_train_verifi/wav
      meta_data: ./downstream/speaker_verifi/dev_meta_data/dev_meta_data.txt
      max_timestep: 131200
    
    train_batch_size: 10
    eval_batch_size: 1
    num_workers: 7 # need to be you total cpu count -1, because we will open a process to load data into memory (chunk).

  modelrc:
    module:
      Identity                                   # downstream model option: TransformerEncoder, Identity, 
      #If set TransformerEncoder, it will follow the hparams to set the transformer encoder and compile it to  be the downstream model, and Identity will simply pass the pretrained feature.
    hparams:
      hidden_size: 256                                      # Size of the encoder layers and the pooler layer.
      num_hidden_layers: 1                                  # Number of hidden layers in the Transformer encoder.
      num_attention_heads: 4                               # Number of attention heads for each attention layer in the Transformer encoder.
      intermediate_size: 1024                               # The size of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
      hidden_act: "gelu"                                    # The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu", "relu" and "swish" are supported.
      hidden_dropout_prob: 0.1                              # The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.
      attention_probs_dropout_prob: 0.1                     # The dropout ratio for the attention probabilities.
      initializer_range: 0.02                               # The sttdev of the truncated_normal_initializer for initializing all weight matrices.
      layer_norm_eps: 1.0e-12                              # The epsilon used by LayerNorm.
      share_layer: False                                    # Share layer weights
      max_input_length: 0                                   # maximum input length (0 for no restriction)
      pre_layer_norm: False                                 # apply the pre layer normalization technique introduced in: https://arxiv.org/abs/2002.04745
    input_dim: 256
    agg_module: SAP                                         # in the end before GE2E, will aggreate frame-level representation to utterance-level representaiont by self-attention-pooling (SAP) or mean pooing (Mean)
